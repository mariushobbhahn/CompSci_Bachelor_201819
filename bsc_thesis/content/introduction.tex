\chapter{Introduction}
\label{chap:introduction}

Many safety critical application require a visual understanding of the scene. For example, in autonomous driving we need to know what objects are on the road. In the task of object detection, objects are enclosed by rectangular bounding boxes and an object detection model is tasked with predicting these given an image. Even though the task is very easy for humans in most situations it is very hard for computers for two main reasons: Firstly, human visual systems have evolved over millions of years and have become very efficient at identifying important information quickly and processing them in an efficient manner. Secondly, a big part of object detection is incorporating prior knowledge, e.g. knowing what a car looks like in different circumstances and having abstract semantic knowledge of objects. It is unclear to what extend artificial systems are able to learn semantic concepts on this level of abstraction. However, in recent years computer vision algorithms have gotten significantly better for a number of reasons. Algorithms, especially in Deep Learning (DL), have evolved quickly and increased the accuracy for many complicated visual tasks drastically. Additionally, hardware has become faster and more algorithms can be partly or entirely parallelized, making it possible to run them on GPUs increasing their computation speed. Lastly, more and more big datasets have become publicly available. Since many DL algorithms need many samples to train on, having more and bigger datasets was an important pillar for their success.

for many different applications like face recognition, object tracking (e.g. the ball in a football match) and especially semantic segmentation of traffic scenes, pedestrian and car tracking. 
%TODO need references for algorithms and use cases

As outlined in the first paragraph many DL algorithms often rely on big datasets to train on. These might not always exist or are very expensive to annotate. Some diseases, for example, are rare and therefore only a few datapoints exist. Medical studies are often very expensive and can therefore only feature a couple of hundred patients. Additionally, at least for visual tasks, the data needs to be annotated. This is paid human labor that can cost a company much money. Over all it is therefore important to find methods that reduce the number of training samples while keeping the high accuracies of current state of the art methods. 
One possible way to decrease the number of samples is to incorporate prior knowledge. This could be done by using theoretical properties, like invariances or equivariances w.r.t. specific transformations and combine them with DL. 
A possible way to realize this is shown by J. Bruna and
S. Mallat \cite{scatteringTransform2012} with the introduction of the Scattering Transform. The Scattering Transform is a new technique that uses wavelet operations on the image by using a static filter that requires no training. This is important because state of the art (SOTA) object detection algorithm use convolutional neural networks (CNNs). Filters used in those CNNs are all trained during the training period which costs time and energy.
J. Bruna and S. Mallat \cite{scatteringTransform2012} apply the Scattering Transform to images and perform classification tasks on their outputs. They also show that the technique is essentially equivalent to using CNNs with fixed weights for some or all filters. 
The reason why the Scattering Transform has proven so successful are the properties it provides. It is invariant to deformation and equivariant to translation. Equivariance is fulfilled for an algorithm if the output and the input are changed in a corresponding fashion when a transformation is applied. For object detection invariance with respect to translation is an undesirable property while equivariance is desired. Whether an object is on the left side of the image or the right one should be reflected in the outcome of the algorithm.  In this work it is also argued that it has desirable properties w.r.t rotation and scaling, i.e. that the scattering transform is able to reproduce the size or rotation of an object in its output. These properties are important for image classification but also necessary for object detection. For example, when detecting pedestrians in real traffic situations, the object detection algorithm must be able to identify them independent of their location, size or rotation within the image while also indicating those properties. A pedestrian that is close must be encoded differently from a pedestrian that is far away or otherwise the car runs her over or breaks without necessity. \\
The usage of static filters provides three specific advantages beyond their transformation properties. First, the scattering transform might yield information that was currently not available to the network and therefore increasing its accuracy. Even if that might only be a marginal increase, it is meaningful for application. Every little reduction of the error in object detection, especially for autonomous driving, means a reduction of risk of self driving cars. This is directly translated to lives being saved in the longterm. 
Second, fixed weights imply no additional training time for them. If, for example, one layer can be substituted that would reduce the length of training and save cost and energy while creating access for people who currently do not own multiple GPUs.
Third, fixed weights cannot be overfit and are maximally general. This might produce more robust algorithms and protect against black box attacks or other malicious practices applied to CNNs. This, however, will not be tested within the scope of this work but might be interesting follow-up.\\
This work investigates the useful properties of the scattering transform and combines it with already established state of the art object detection algorithms. This is be done primarily in two ways. First, the techniques are combined sequentially, i.e. the SOTA algorithms are applied only to the outputs of the scattering transform. \cite{ScalingTheScatteringTransform2017}
have already shown that sequential combination is able to produce SOTA results for image recognition. This is the attempt to extend these findings for object detection. \\
Second, the techniques are combined in parallel, i.e. the information of the scattering transform are used as additional inputs for the object detection algorithm or merged at later stages. This has not been tested yet and is the primary extension of related work described in the following section.\\
The paper is separated in three main parts. A Theory chapter which presents an introduction to the techniques used in this paper and a more detailed analysis of the Scattering Transform. It is followed by an Experiment chapter which contains a detailed overview of the experimental setups for this work. Lastly, the Results chapter analyzes the experimental results and discusses them. 


\section{Related Work}

The Scattering Transform has been applied successfully in many different classification tasks. \cite{InvariantScatteringTextureDiscrimination2013} showed that the scattering transform is applicable to texture discrimination. \cite{DeepRotoTranslation2014} have demonstrated that the scattering transform also produces results similar to other SOTA algorithms for unsupervised learning. \cite{3DScatteringTransformNeuro2017} improved the classification of diseases from neuroimages considerably. Lastly, \cite{ScalingTheScatteringTransform2017} shows that substituting the first layer filters of CNN approaches with the scattering transform yields equivalent results compared to these filters being trained.\\